{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training\n",
    "\n",
    "We will use a `Trainer` class to perform the training and validation experiments. It requires a configuration file where all training and experiment parameters are defined in hierarchical structure. The configuration is usually a `.yaml` file which is converted to hierarchical configurations using [Hydra](https://hydra.cc/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training configuration\n",
    "The default configurations is specified in `config/base.yaml`. To override the default values using class indexing, for instance, to change the `batch_size`, use `train.batch_size=32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "cfg = OmegaConf.create(OmegaConf.load(\"./configs/base.yaml\"))\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization\n",
    "\n",
    "The initialization of the Trainer class will create a model for training based on the network specified in the `.yaml` file, define loss function prepare an output directory `<cfg.exp.log_dir>/<exp_name>`, to save model checkpoints, and logs training and prediction outputs. If you don't want to log anything, you can set the parameter `train.debug` of the hydra config to `False`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "\n",
    "trainer = Trainer(cfg, exp_name='exp1_training')\n",
    "print(trainer.network)\n",
    "print(\"Training parameters:\", trainer.num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for training and evaluation\n",
    "\n",
    "Next, we obtain the list of shots ids with available labels from the `data.label_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.datasets import split_data\n",
    "from utils.misc import get_files_in_dir\n",
    "\n",
    "data = [f.split('.')[0] for f in get_files_in_dir(cfg.data.label_dir, file_end='.csv')]\n",
    "\n",
    "train_shots, test_shots = split_data(data, train_split=cfg.data.train_split)\n",
    "print(f\"{data=}\\n{train_shots=}\\n{test_shots=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train function\n",
    "Next we call the `train` function of the `Trainer` with training and test shots ID. The `train` function initialize dataloders, create model checkpointer for saving model with best metric and schedulers for adjusting learning rate. Note that, the training will monitor the metric specified in `train.monitor` to save the model checkpoints depending upon the monitor mode e.g. 'min' for 'loss' and 'max' for 'accuracy'.  The trainer utilize EarlyStopping method to stop the training if  metric specified in the `train.early_stopping_metric` doesn't improve for `train.early_stop_patience` steps.  \n",
    "\n",
    "The trainer will create a model for training based on the network specified in the config file, train the network and save the network states in a output directory `<cfg.exp.log_dir>/<exp_name>`,  where in addition to model checkpoints, logs and any prediction outputs will be saved. If you don't want to log anything, you can set the parameter `train.debug` of the hydra config to `False`.\n",
    "\n",
    "By default, the model is trained to predict the ELM types classification only. If ELM detection is required (identifying where are the ELM peaks), you need to set the `net.detection=True`, which will train a separate detection head for detections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_shots, test_sets=test_shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the test results\n",
    "print(\"Classification results:\\n\", trainer.cls_metric_logger.tabulate_metrics())\n",
    "if cfg.net.detection:\n",
    "    print(\"Detection results:\\n\", trainer.det_metric_logger.tabulate_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "trainer.cls_metric_logger.plot_confusion_matrix()\n",
    "if cfg.net.detection:\n",
    "    trainer.det_metric_logger.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ELM Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.datasets import ELMDataset\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "shot_id = []\n",
    "\n",
    "h = trainer.evaluate(['30462'], phase='eval')\n",
    "trainer.plot_preds(phase='eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the progress in Tensoboard\n",
    "\n",
    "One a new terminal, launch tensoboard with log dir set to `./logs`.\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir ./logs/ --bind_all\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Fold Validation\n",
    "\n",
    "We will use the same steps as in the previous secsion, but this time we will rotate the validation set for K times and run the training in each set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from utils.misc import get_files_in_dir\n",
    "from dataset.datasets import split_data\n",
    "from trainer import Trainer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = OmegaConf.create(OmegaConf.load(\"./configs/base.yaml\"))\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "data = [f.split('.')[0] for f in get_files_in_dir(cfg.data.label_dir, file_end='.csv')]\n",
    "kfolds_data = split_data(data, n_folds=n_folds)\n",
    "\n",
    "kfold_cls_results = []\n",
    "kfold_det_results = []\n",
    "\n",
    "for i in range(n_folds):\n",
    "    \n",
    "    # Create test samples for this fold\n",
    "    train_shots, test_shots = kfolds_data[i]\n",
    "    print(f\"Fold={i}/{n_folds} \\n{train_shots=}\\n{test_shots=}\")\n",
    "\n",
    "    # create a trainer and train on each fold \n",
    "    kfold_trainer = Trainer(cfg, exp_name=f\"exp1_{n_folds}folds_training/fold{i+1}\")\n",
    "    kfold_trainer.train(train_shots, test_sets=test_shots)\n",
    "\n",
    "    kfold_cls_results.append(kfold_trainer.cls_metric_logger.results)\n",
    "    if cfg.net.detection:\n",
    "        kfold_det_results.append(kfold_trainer.det_metric_logger.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize K-Fold Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for k, data in enumerate(kfolds_data):\n",
    "    print(f\"Fold {k+1}: train shots={data[0]} test shots={data[1]}\")\n",
    "\n",
    "kfold_cls_results_df = pd.DataFrame(kfold_cls_results)\n",
    "print(\"K-Fold Results: \\n\", kfold_cls_results_df)\n",
    "sum_metrics = kfold_cls_results_df[[\"tp\", \"fp\", \"tn\", \"fn\"]].sum()\n",
    "mean_metrics = kfold_cls_results_df[[\"accuracy\", \"precision\", \"recall\", \"f1\"]].mean()\n",
    "avg_results = pd.DataFrame([sum_metrics.tolist() + mean_metrics.tolist()], \n",
    "                           columns=sum_metrics.index.tolist() + mean_metrics.index.tolist()\n",
    "                          )\n",
    "print(\"Average K-Fold Results: \\n\", avg_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vis_utils import plot_bar_metrics\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "plot_bar_metrics(kfold_cls_results, xticks_label='Fold', figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_metrics(kfold_det_results, metric_type='Detection', xticks_label='Fold', figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3. Active Learning with Random Sampling\n",
    "For active learning, we will use the same config `base.yaml`. We will incrementally add the training shots as we go through the AL iterations. Due to limited available labels, we will keep the same test shots for evaluation at each iteration. We can expect increasing accuracy over AL iterationss and higher accuracy than the normal and fold 5 of the K-fold training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from utils.misc import get_files_in_dir\n",
    "from dataset.datasets import split_data\n",
    "from trainer import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "cfg = OmegaConf.create(OmegaConf.load(\"./configs/base.yaml\"))\n",
    "\n",
    "data = [f.split('.')[0] for f in get_files_in_dir(cfg.data.label_dir, file_end='.csv')]\n",
    "train_shots, test_shots = split_data(data, train_split=cfg.data.train_split)\n",
    "print(f\"{data=}\\n{train_shots=}\\n{test_shots=}\")\n",
    "\n",
    "al_trainer = Trainer(cfg, exp_name='active_learning_with_random_sampling')\n",
    "\n",
    "rng = np.random.default_rng(cfg.rng.seed)\n",
    "\n",
    "# Active learning Options\n",
    "INITIAL_LABELS = 5\n",
    "N_CYCLES = 5\n",
    "QUERY_BATCH_SIZE = 2\n",
    "\n",
    "# Split labeled and unlabeled data\n",
    "labeled_shots = []\n",
    "unlabeled_shots = train_shots\n",
    "\n",
    "al_cls_results = []\n",
    "al_det_results = []\n",
    "for i in range(N_CYCLES):\n",
    "    \n",
    "    print(f\"\\nActive Learning Cycle {i + 1}\")\n",
    "    \n",
    "    # Random sample from unlabelled indices\n",
    "    selected_shots = rng.choice(unlabeled_shots, \n",
    "                                size=QUERY_BATCH_SIZE, \n",
    "                                replace=False,\n",
    "                                ).tolist()\n",
    "\n",
    "    labeled_shots.extend(selected_shots)\n",
    "    \n",
    "    unlabeled_shots = [v for v in unlabeled_shots if v not in selected_shots]\n",
    "    \n",
    "    print(f\"{labeled_shots=}\\n{unlabeled_shots=}\")\n",
    "\n",
    "    al_trainer.train(train_sets=labeled_shots,\n",
    "                     test_sets=test_shots,\n",
    "                    )\n",
    "    al_trainer.save_states(ckpt_name=f\"model_states_cycle{i}\")\n",
    "    \n",
    "    al_cls_results.append(al_trainer.cls_metric_logger.results)\n",
    "    if cfg.net.detection:\n",
    "        al_det_results.append(al_trainer.det_metric_logger.results)\n",
    "\n",
    "    if not len(unlabeled_shots)>0:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize results of Active Learning with Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from utils.vis_utils import plot_bar_metrics\n",
    "# Visualize classification results\n",
    "plot_bar_metrics(al_cls_results, metric_type='classification', xticks_label='Iteration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detection results\n",
    "if len(al_det_results)>0:\n",
    "    plot_bar_metrics(al_det_results, metric_type='Detection', xticks_label='Iteration')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Active learning with Uncertainity Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.misc import get_files_in_dir\n",
    "from dataset.datasets import split_data, collate_fn\n",
    "from dataset.datasets import ELMDataset\n",
    "from trainer import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "INITIAL_LABELS = 5\n",
    "QUERY_BATCH_SIZE = 5\n",
    "N_CYCLES = 5\n",
    "\n",
    "def compute_entropy(model, dataset, device):\n",
    "    model.eval()\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=1,\n",
    "        collate_fn=collate_fn,\n",
    "        )\n",
    "\n",
    "    cls_uncertainties = []\n",
    "    det_uncertainities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            cls_preds, elm_preds = model(batch.dalpha)\n",
    "            cls_probs = torch.softmax(cls_preds, dim=1).cpu().numpy()\n",
    "            cls_ent = entropy(cls_probs, axis=1)\n",
    "            cls_uncertainties.append(cls_ent.mean())\n",
    "            if elm_preds is not None:\n",
    "                det_probs = torch.softmax(elm_preds, dim=1).cpu().numpy()\n",
    "                det_ent = entropy(det_probs, axis=1)\n",
    "                det_uncertainities.append(det_ent.mean())\n",
    "\n",
    "    \n",
    "    return cls_uncertainties, det_uncertainities\n",
    "\n",
    "def al_uncertainity_iterations(cfg, train_shots, test_shots, initial_shots, exp_name='al_uncertainity_sampling'):\n",
    "    \n",
    "    # Create the trainer\n",
    "    trainer = Trainer(cfg, exp_name=exp_name)\n",
    "\n",
    "    labeled_shots = []\n",
    "    unlabeled_shots = train_shots\n",
    "    selected_shots = initial_shots\n",
    "\n",
    "    cls_results = []\n",
    "    det_results = []\n",
    "    for i in range(N_CYCLES):\n",
    "        \n",
    "        print(f\"\\nActive Learning Cycle {i + 1}\")\n",
    "        \n",
    "        labeled_shots.extend(selected_shots)\n",
    "        \n",
    "        unlabeled_shots = [s for s in unlabeled_shots if s not in selected_shots]\n",
    "    \n",
    "        print(f\"{labeled_shots=}\\n{unlabeled_shots=}\")\n",
    "        \n",
    "        trainer.train(train_shots, test_sets=test_shots,)\n",
    "            \n",
    "        # create dataset for unlabelled indices\n",
    "        unlabeled_dataset = ELMDataset(cfg.data, \n",
    "                                       label_files=unlabeled_shots, \n",
    "                                       mode='test',\n",
    "                                      )\n",
    "    \n",
    "        cls_uncertainty_scores, det_uncertainity_scores = compute_entropy(\n",
    "            model=trainer.network, \n",
    "            dataset=unlabeled_dataset, \n",
    "            device=trainer.device,\n",
    "            )\n",
    "    \n",
    "        # Select most uncertain samples\n",
    "        query_indices = np.argsort(cls_uncertainty_scores)[-QUERY_BATCH_SIZE:]\n",
    "    \n",
    "        # Add the uncertain samples for training\n",
    "        selected_shots = [unlabeled_shots[idx] for idx in query_indices]\n",
    "        \n",
    "        cls_results.append(trainer.cls_metric_logger.results)\n",
    "        if cfg.net.detection:\n",
    "            det_results.append(trainer.det_metric_logger.results)\n",
    "    \n",
    "        if not len(unlabeled_shots)>0:\n",
    "            break\n",
    "\n",
    "    return trainer, cls_results, det_results \n",
    "\n",
    "cfg = OmegaConf.create(OmegaConf.load(\"./configs/base.yaml\"))\n",
    "\n",
    "# Load train/test data\n",
    "shots = [f.split('.')[0] for f in get_files_in_dir(cfg.data.label_dir, file_end='.csv')]\n",
    "train_shots, test_shots = split_data(shots, train_split=cfg.data.train_split, seed=cfg.rng.seed)\n",
    "print(f\"{shots=}\\n{train_shots=}\\n{test_shots=}\")\n",
    "\n",
    "# Initialize labeled and unlabeled samples\n",
    "# rng = np.random.default_rng(cfg.rng.seed)\n",
    "\n",
    "# selected_shots = rng.choice(train_shots, \n",
    "#                             size=INITIAL_LABELS, \n",
    "#                             replace=False,\n",
    "#                             ).tolist()\n",
    "\n",
    "initial_shots = ['30418', '30424', '30441', '30449', '30457']\n",
    "\n",
    "ent_trainer, al_ent_cls_results, al_ent_det_results = al_uncertainity_iterations(\n",
    "    cfg, \n",
    "    train_shots, \n",
    "    test_shots, \n",
    "    initial_shots,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification resultsimport matplotlib \n",
    "%matplotlib inline\n",
    "\n",
    "from utils.vis_utils import plot_bar_metrics\n",
    "\n",
    "plot_bar_metrics(al_ent_cls_results, metric_type='classification', xticks_label='Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detection results\n",
    "if len(al_ent_det_results)>0:\n",
    "    plot_bar_metrics(al_ent_det_results, metric_type='Detection', xticks_label='Iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Active Learning with K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from utils.misc import get_files_in_dir\n",
    "from dataset.datasets import split_data\n",
    "from trainer import Trainer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = OmegaConf.create(OmegaConf.load(\"./configs/base.yaml\"))\n",
    "\n",
    "INITIAL_LABELS = 5\n",
    "QUERY_BATCH_SIZE = 5\n",
    "N_CYCLES = 5\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "data = [f.split('.')[0] for f in get_files_in_dir(cfg.data.label_dir, file_end='.csv')]\n",
    "kfolds_data = split_data(data, n_folds=n_folds)\n",
    "\n",
    "kfold_al_cls_results = []\n",
    "kfold_al_det_results = []\n",
    "\n",
    "rng = np.random.default_rng(cfg.rng.seed)\n",
    "\n",
    "for i in range(n_folds):\n",
    "    \n",
    "    # Create test samples for this fold\n",
    "    train_shots, \n",
    "    = kfolds_data[i]\n",
    "    print(f\"Fold={i}/{n_folds} \\n{train_shots=}\\n{test_shots=}\")\n",
    "\n",
    "    initial_shots = rng.choice(train_shots, \n",
    "                                size=INITIAL_LABELS, \n",
    "                                replace=False,\n",
    "                                ).tolist()\n",
    "\n",
    "    _, _kfold_al_cls_results, _kfold_al_det_results = al_uncertainity_iterations(\n",
    "        cfg, \n",
    "        train_shots, \n",
    "        test_shots, \n",
    "        initial_shots,\n",
    "    )\n",
    "\n",
    "    kfold_al_cls_results.append(_kfold_al_cls_results)\n",
    "    kfold_al_det_results.append(_kfold_al_det_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, fold_results in enumerate(kfold_al_cls_results):\n",
    "    plot_bar_metrics(fold_results, metric_type=f'Classification (Fold-{i+1})', xticks_label='Iteration')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvis",
   "language": "python",
   "name": "mlvis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
