{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training\n",
    "\n",
    "We will use a `Trainer` class to perform the training and validation experiments. It requires a configuration file where all training and experiment parameters are defined in hierarchical structure. The configuration is usually a `.yaml` file which is converted to hierarchical configurations using [Hydra](https://hydra.cc/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training configuration\n",
    "The default configurations is specified in `config/base.yaml`. To override the default values using class indexing, for instance, to change the `batch_size`, use `train.batch_size=32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "cfg = OmegaConf.create(OmegaConf.load(\"./configs/base.yaml\"))\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization\n",
    "\n",
    "The initialization of the Trainer class will create a model for training based on the network specified in the `.yaml` file, define loss function prepare an output directory `<cfg.exp.log_dir>/<exp_name>`, to save model checkpoints, and logs training and prediction outputs. If you don't want to log anything, you can set the parameter `train.debug` of the hydra config to `False`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "\n",
    "trainer = Trainer(cfg, exp_name='exp1_training')\n",
    "print(trainer.network)\n",
    "print(\"Training parameters:\", trainer.num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for training and evaluation\n",
    "\n",
    "Next, we obtain the list of shots ids with available labels from the `data.label_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.datasets import split_data\n",
    "from utils.misc import get_files_in_dir\n",
    "\n",
    "data = [f.split('.')[0] for f in get_files_in_dir(cfg.data.label_dir, file_end='.csv')]\n",
    "\n",
    "train_shots, test_shots = split_data(data, train_split=cfg.data.train_split)\n",
    "print(f\"{data=}\\n{train_shots=}\\n{test_shots=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train function\n",
    "Next we call the `train` function of the `Trainer` with training and test shots ID. The `train` function initialize dataloders, create model checkpointer for saving model with best metric and schedulers for adjusting learning rate. Note that, the training will monitor the metric specified in `train.monitor` to save the model checkpoints depending upon the monitor mode e.g. 'min' for 'loss' and 'max' for 'accuracy'.  The trainer utilize EarlyStopping method to stop the training if  metric specified in the `train.early_stopping_metric` doesn't improve for `train.early_stop_patience` steps.  \n",
    "\n",
    "The trainer will create a model for training based on the network specified in the config file, train the network and save the network states in a output directory `<cfg.exp.log_dir>/<exp_name>`,  where in addition to model checkpoints, logs and any prediction outputs will be saved. If you don't want to log anything, you can set the parameter `train.debug` of the hydra config to `False`.\n",
    "\n",
    "By default, the model is trained to predict the ELM types classification only. If ELM detection is required (identifying where are the ELM peaks), you need to set the `net.detection=True`, which will train a separate detection head for detections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_shots, test_sets=test_shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the test results\n",
    "print(\"Classification results:\\n\", trainer.cls_metric_logger.tabulate_metrics())\n",
    "if cfg.net.detection:\n",
    "    print(\"Detection results:\\n\", trainer.det_metric_logger.tabulate_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "trainer.cls_metric_logger.plot_confusion_matrix()\n",
    "if cfg.net.detection:\n",
    "    trainer.det_metric_logger.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ELM Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.datasets import ELMDataset\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "shot_id = []\n",
    "\n",
    "h = trainer.evaluate(['30462'], phase='eval')\n",
    "trainer.plot_preds(phase='eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the progress in Tensoboard\n",
    "\n",
    "One a new terminal, launch tensoboard with log dir set to `./logs`.\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir ./logs/ --bind_all\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Fold Validation\n",
    "\n",
    "We will use the same steps as in the previous secsion, but this time we will rotate the validation set for K times and run the training in each set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from utils.misc import get_files_in_dir\n",
    "from dataset.datasets import split_data\n",
    "from trainer import Trainer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "with open(\"./configs/base.yaml\", \"r\") as f:\n",
    "    cfg = OmegaConf.create(OmegaConf.load(f))\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "data = [f.split('.')[0] for f in get_files_in_dir(cfg.data.label_dir, file_end='.csv')]\n",
    "kfolds_data = split_data(data, n_folds=n_folds)\n",
    "\n",
    "kfold_cls_results = []\n",
    "kfold_det_results = []\n",
    "\n",
    "for i in range(n_folds):\n",
    "    \n",
    "    # Create test samples for this fold\n",
    "    train_shots, test_shots = kfolds_data[i]\n",
    "    print(f\"Fold={i}/{n_folds} \\n{train_shots=}\\n{test_shots=}\")\n",
    "\n",
    "    # create a trainer and train on each fold \n",
    "    kfold_trainer = Trainer(cfg, exp_name=f\"exp1_{n_folds}folds_training/fold{i+1}\")\n",
    "    kfold_trainer.train(train_shots, test_sets=test_shots)\n",
    "\n",
    "    kfold_cls_results.append(kfold_trainer.cls_metric_logger.results)\n",
    "    if cfg.net.detection:\n",
    "        kfold_det_results.append(kfold_trainer.det_metric_logger.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize K-Fold Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for k, data in enumerate(kfolds_data):\n",
    "    print(f\"Fold {k+1}: train shots={data[0]} test shots={data[1]}\")\n",
    "\n",
    "kfold_cls_results_df = pd.DataFrame(kfold_cls_results)\n",
    "print(\"K-Fold Results: \\n\", kfold_cls_results_df)\n",
    "sum_metrics = kfold_cls_results_df[[\"tp\", \"fp\", \"tn\", \"fn\"]].sum()\n",
    "mean_metrics = kfold_cls_results_df[[\"accuracy\", \"precision\", \"recall\", \"f1\"]].mean()\n",
    "avg_results = pd.DataFrame([sum_metrics.tolist() + mean_metrics.tolist()], \n",
    "                           columns=sum_metrics.index.tolist() + mean_metrics.index.tolist()\n",
    "                          )\n",
    "print(\"Average K-Fold Results: \\n\", avg_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vis_utils import plot_bar_metrics\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot bar chart for ELM Type Classification\n",
    "plot_bar_metrics(kfold_cls_results, xticks_label='Fold', figsize=(5, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar chart for ELM Peak detection\n",
    "plot_bar_metrics(kfold_det_results, figtitle='Detection', xticks_label='Fold', figsize=(5, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3. Active Learning\n",
    "For active learning, we will use the same config `base.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from utils.misc import get_files_in_dir\n",
    "from dataset.datasets import split_data\n",
    "\n",
    "with open(\"./configs/base.yaml\", \"r\") as f:\n",
    "    cfg = OmegaConf.create(OmegaConf.load(f))\n",
    "\n",
    "shots = [f.split('.')[0] for f in get_files_in_dir(cfg.data.label_dir, file_end='.csv')]\n",
    "train_shots, test_shots = split_data(shots, train_split=cfg.data.train_split, seed=cfg.rng.seed)\n",
    "print(f\"{shots=}\\n{train_shots=}\\n{test_shots=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Active Learning Iterations/Steps\n",
    "\n",
    "To mimic the active learning process, we will randomly select the INITIAL_LABELS for the first iteration and incrementally add QUERY_BATCH_SIZE number of unlabelled training shots as the iterations proceeds based on the sampling strategy. The iteration is repeated for N_CYCLES time (or until all shots are labelled). To achieve this we write an `al_iterations` function which currently supports `random` and `uncertainity` sampling strategies. Due to limited available labels, we will keep the same test shots for evaluation at each iteration (later validated using KFold validation). We can expect increasing accuracy over AL iterationss and higher accuracy than the normal and fold 5 of the K-fold training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.vis_utils import plot_bar_metrics\n",
    "from utils.misc import get_files_in_dir\n",
    "from dataset.datasets import split_data, collate_fn\n",
    "from dataset.datasets import ELMDataset\n",
    "from trainer import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Active learning Options\n",
    "INITIAL_LABELS = 4\n",
    "QUERY_BATCH_SIZE = 5\n",
    "N_CYCLES = 5\n",
    "\n",
    "def compute_entropy(model, dataset, device):\n",
    "    model.eval()\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=1,\n",
    "        collate_fn=collate_fn,\n",
    "        )\n",
    "\n",
    "    cls_uncertainties = []\n",
    "    det_uncertainities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            cls_preds, elm_preds = model(batch.dalpha)\n",
    "            cls_probs = torch.softmax(cls_preds, dim=1).cpu().numpy()\n",
    "            cls_ent = entropy(cls_probs, axis=1)\n",
    "            cls_uncertainties.append(cls_ent.mean())\n",
    "            if elm_preds is not None:\n",
    "                det_probs = torch.softmax(elm_preds, dim=1).cpu().numpy()\n",
    "                det_ent = entropy(det_probs, axis=1)\n",
    "                det_uncertainities.append(det_ent.mean())\n",
    "\n",
    "    \n",
    "    return cls_uncertainties, det_uncertainities\n",
    "    \n",
    "def al_iterations(trainer, train_shots, test_shots, sampling_strategy='random'):\n",
    "\n",
    "    labeled_shots = []\n",
    "    unlabeled_shots = train_shots\n",
    "    # Randomly select initial shots\n",
    "    selected_shots = trainer.rng_gen.choice(\n",
    "        train_shots, \n",
    "        size=INITIAL_LABELS, \n",
    "        replace=False,\n",
    "        ).tolist()\n",
    "    \n",
    "    cls_results = []\n",
    "    det_results = []\n",
    "    for i in range(N_CYCLES):\n",
    "        \n",
    "        print(f\"\\nActive Learning Cycle {i + 1}\")\n",
    "        \n",
    "        labeled_shots.extend(selected_shots)\n",
    "        \n",
    "        unlabeled_shots = [v for v in unlabeled_shots if v not in selected_shots]\n",
    "        \n",
    "        print(f\"{labeled_shots=}\\n{unlabeled_shots=}\")\n",
    "\n",
    "        # train\n",
    "        trainer.train(labeled_shots, test_sets=test_shots)\n",
    "        trainer.save_states(ckpt_name=f\"model_states_cycle{i}\")\n",
    "\n",
    "        cls_results.append(trainer.cls_metric_logger.results)\n",
    "        if trainer.cfg.net.detection:\n",
    "            det_results.append(trainer.det_metric_logger.results)\n",
    "            \n",
    "        if len(unlabeled_shots)>QUERY_BATCH_SIZE-1:\n",
    "\n",
    "            if sampling_strategy=='uncertainity':\n",
    "                # create dataset for unlabelled indices\n",
    "                unlabeled_dataset = ELMDataset(trainer.cfg.data, \n",
    "                                               label_files=unlabeled_shots, \n",
    "                                               mode='test',\n",
    "                                              )\n",
    "            \n",
    "                cls_uncertainty_scores, det_uncertainity_scores = compute_entropy(\n",
    "                    model=trainer.network, \n",
    "                    dataset=unlabeled_dataset, \n",
    "                    device=trainer.device,\n",
    "                    )\n",
    "            \n",
    "                # Select most uncertain samples\n",
    "                query_indices = np.argsort(cls_uncertainty_scores)[-QUERY_BATCH_SIZE:]\n",
    "            \n",
    "                # Add the uncertain samples for training\n",
    "                selected_shots = [unlabeled_shots[idx] for idx in query_indices]\n",
    "            \n",
    "            elif sampling_strategy=='random':\n",
    "                selected_shots = trainer.rng_gen.choice(\n",
    "                    unlabeled_shots, \n",
    "                    size=QUERY_BATCH_SIZE, \n",
    "                    replace=False,\n",
    "                    ).tolist()\n",
    "            else:\n",
    "                raise Exception(f\"sampling strategy {sampling} is not supported.\")\n",
    "            \n",
    "    return cls_results, det_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Active Learning with Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_random_trainer = Trainer(cfg, exp_name='al_random_sampling')\n",
    "\n",
    "al_random_cls_results, al_random_det_results = al_iterations(\n",
    "    al_random_trainer,\n",
    "    train_shots,\n",
    "    test_shots, \n",
    "    sampling_strategy='random',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# Visualize classification results \n",
    "plot_bar_metrics(al_random_cls_results, \n",
    "                 figtitle='Classification (Random Sampling)', \n",
    "                 xticks_label='Iteration', \n",
    "                 figsize=(5, 4),\n",
    "                 save_dir=al_random_trainer.exp_dir,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detection results\n",
    "if len(al_random_det_results)>0:\n",
    "    plot_bar_metrics(al_random_det_results, \n",
    "                     figtitle='Detection (Random Sampling)', \n",
    "                     xticks_label='Iteration', \n",
    "                     figsize=(5, 4),\n",
    "                     save_dir=al_random_trainer.exp_dir,\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Active learning with Uncertainity Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_uncertainity_trainer = Trainer(cfg, exp_name='al_uncertainity_sampling')\n",
    "\n",
    "al_uncertainity_cls_results, al_uncertainity_det_results = al_iterations(\n",
    "    al_uncertainity_trainer,\n",
    "    train_shots, \n",
    "    test_shots, \n",
    "    sampling_strategy='uncertainity',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "%matplotlib inline\n",
    "\n",
    "plot_bar_metrics(al_uncertainity_cls_results, \n",
    "                 figtitle='Classification (Uncertainity Sampling)', \n",
    "                 xticks_label='Iteration',\n",
    "                 figsize=(5,4),\n",
    "                 save_dir=al_uncertainity_trainer.exp_dir,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detection results\n",
    "if len(al_uncertainity_det_results)>0:\n",
    "    plot_bar_metrics(al_uncertainity_det_results, \n",
    "                     figtitle='Detection (Uncertainity Sampling)', \n",
    "                     xticks_label='Iteration',\n",
    "                     figsize=(5,4),\n",
    "                     save_dir=al_uncertainity_trainer.exp_dir,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Active Learning with K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from utils.misc import get_files_in_dir\n",
    "from dataset.datasets import split_data\n",
    "from trainer import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "with open(\"./configs/base.yaml\", \"r\") as f:\n",
    "    cfg = OmegaConf.create(OmegaConf.load(f))\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "data = [f.split('.')[0] for f in get_files_in_dir(cfg.data.label_dir, file_end='.csv')]\n",
    "kfolds_data = split_data(data, n_folds=n_folds)\n",
    "\n",
    "kfold_al_cls_results = []\n",
    "kfold_al_det_results = []\n",
    "\n",
    "for i in range(n_folds):\n",
    "    \n",
    "    # Create test samples for this fold\n",
    "    train_shots, test_shots = kfolds_data[i]\n",
    "    print(f\"Fold={i}/{n_folds} \\n{train_shots=}\\n{test_shots=}\")\n",
    "\n",
    "    kfold_al_trainer = Trainer(cfg, exp_name=f'kfold_al_uncertainity_sampling/fold{i+1}')\n",
    "\n",
    "    _cls_results, _det_results = al_iterations(\n",
    "        kfold_al_trainer,\n",
    "        train_shots, \n",
    "        test_shots, \n",
    "        sampling_strategy='uncertainity',\n",
    "    )\n",
    "\n",
    "    plot_bar_metrics(_cls_results, \n",
    "                     figtitle=f'Classification (Fold-{i+1})', \n",
    "                     xticks_label='Iteration', \n",
    "                     save_dir=kfold_al_trainer.exp_dir,\n",
    "                    )\n",
    "    if cfg.net.detection:\n",
    "        plot_bar_metrics(_det_results, \n",
    "                         figtitle=f'Detection (Fold-{i+1})', \n",
    "                         xticks_label='Iteration', \n",
    "                         save_dir=kfold_al_trainer.exp_dir,\n",
    "                        )\n",
    "    \n",
    "    kfold_al_cls_results.append(_cls_results)\n",
    "    kfold_al_det_results.append(_det_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the KFold Active Leraning results\n",
    "import math\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(len(kfold_al_cls_results)/n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 8))\n",
    "axes_flat = np.ravel(axes)\n",
    "\n",
    "# Visualize the K-fold results in each AL Iterations\n",
    "for i, fold_results in enumerate(kfold_al_cls_results):\n",
    "    plot_bar_metrics(fold_results,\n",
    "                     ax=axes_flat[i],\n",
    "                     figtitle=f'Classification (Fold-{i+1})', \n",
    "                     xticks_label='Iteration',\n",
    "                    )\n",
    "\n",
    "axes_flat[-1].axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_flat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvis",
   "language": "python",
   "name": "mlvis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
